# Speech-based Grammar Score Prediction ğŸ¤ğŸ“Š

> **Competition**: SHL-Internship Assessment  
> **Task**: Predict continuous grammar scores (1-5) from spoken audio samples  
> **Evaluation Metrics**: Pearson Correlation & RMSE

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Problem Statement](#problem-statement)
- [Solution Approach](#solution-approach)
- [Architecture](#architecture)
- [Results](#results)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Model Details](#model-details)
- [Training Details](#training-details)
- [Inference](#inference)
- [Evaluation](#evaluation)
- [Future Improvements](#future-improvements)
- [Acknowledgments](#acknowledgments)

---

## ğŸ¯ Overview

This project implements a **multimodal deep learning solution** for automated grammar scoring of spoken English audio samples. The system combines acoustic features from audio and linguistic features from text transcripts to predict continuous grammar scores ranging from 1 (poor) to 5 (excellent).

### Key Features

âœ… **Multimodal Architecture** - Fuses audio (WavLM) and text (BERT) features  
âœ… **Advanced Fusion** - Bidirectional cross-attention mechanism  
âœ… **Robust Training** - 5-fold cross-validation with ensemble  
âœ… **State-of-the-art Techniques** - SWA, mixed precision, advanced augmentation  
âœ… **Production-Ready** - Complete preprocessing and inference pipeline  

---

## ğŸ“ Problem Statement

### Grammar Score Rubric

| Score | Description |
|-------|-------------|
| **1** | Limited control over sentence structure and syntax; struggles with basic grammatical structures |
| **2** | Limited understanding with consistent basic mistakes; may leave sentences incomplete |
| **3** | Decent grasp of sentence structure with grammatical errors, or vice versa |
| **4** | Strong understanding with good control; occasional minor errors that don't cause misunderstandings |
| **5** | High grammatical accuracy with adept control of complex structures; seldom makes noticeable mistakes |

### Dataset

- **Training**: 409 audio samples (45-60 seconds each)
- **Testing**: 197 audio samples
- **Format**: WAV audio files + CSV transcripts
- **Labels**: Continuous MOS Likert Grammar Scores (1-5)

---

## ğŸš€ Solution Approach

### 1. Multimodal Architecture

Our solution leverages **two complementary modalities**:

#### ğŸµ Audio Branch
- **Model**: Microsoft WavLM-base (94M parameters)
- **Purpose**: Captures prosody, fluency, speech patterns, and acoustic cues
- **Features**: Multi-head attention pooling over temporal features

#### ğŸ“ Text Branch
- **Model**: BERT-base-uncased (110M parameters)
- **Purpose**: Analyzes grammar, syntax, and linguistic structure
- **Features**: [CLS] token representation with contextual embeddings

#### ğŸ”€ Fusion Layer
- **Mechanism**: Bidirectional cross-attention (8 heads)
- **Innovation**: Gated fusion learns optimal combination weights
- **Output**: 768-dimensional multimodal representation

#### ğŸ¯ Regression Head
- **Architecture**: Deep 6-layer MLP (768â†’512â†’384â†’256â†’128â†’64â†’1)
- **Regularization**: Layer normalization, GELU activation, dropout
- **Output**: Continuous score scaled to [1, 5]

### 2. Training Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5-Fold Cross-Validation                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Stratified splits for balanced distribution          â”‚
â”‚  â€¢ 25 epochs per fold with early stopping               â”‚
â”‚  â€¢ Weighted ensemble by Pearson correlation             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Advanced Optimization                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ AdamW optimizer (lr=3e-5, wd=0.02)                   â”‚
â”‚  â€¢ OneCycleLR scheduler with warmup                     â”‚
â”‚  â€¢ Gradient accumulation (effective batch size: 12)     â”‚
â”‚  â€¢ Mixed precision training (FP16)                      â”‚
â”‚  â€¢ Gradient clipping (max norm: 1.0)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Regularization Techniques                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Stochastic Weight Averaging (from epoch 12)          â”‚
â”‚  â€¢ Layer freezing (first 4 layers of encoders)          â”‚
â”‚  â€¢ Dropout (0.1-0.3 throughout network)                 â”‚
â”‚  â€¢ Early stopping (patience: 8, min_delta: 0.0001)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Augmentation                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Time stretching (0.9-1.1x)                           â”‚
â”‚  â€¢ Pitch shifting (Â±5%)                                 â”‚
â”‚  â€¢ Gaussian noise injection                             â”‚
â”‚  â€¢ Time shifting (Â±15%)                                 â”‚
â”‚  â€¢ Volume perturbation (0.8-1.2x)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Custom Loss Function

**RMSE-Focused Loss** = 0.5 Ã— MSE + 0.3 Ã— Huber + 0.2 Ã— Ordinal

- **MSE**: Direct RMSE optimization
- **Huber**: Robustness to outliers (Î´=0.5)
- **Ordinal**: Encourages correct score ordering

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        INPUT LAYER                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Audio: (batch, 160000)     Text: (batch, 512)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WavLM-base     â”‚                       â”‚   BERT-base     â”‚
â”‚  (Frozen: 0-3)  â”‚                       â”‚  (Frozen: 0-3)  â”‚
â”‚  768-dim output â”‚                       â”‚  768-dim output â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                          â”‚
        â”‚ Multi-head                               â”‚ [CLS]
        â”‚ Attention Pool                           â”‚ Token
        â”‚ (8 heads)                                â”‚
        â”‚                                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Projection     â”‚                       â”‚  Projection     â”‚
â”‚  768â†’512â†’384    â”‚                       â”‚  768â†’512â†’384    â”‚
â”‚  [LN+GELU+Drop] â”‚                       â”‚  [LN+GELU+Drop] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    Bidirectional Cross-Attention        â”‚
        â”‚    (8 heads, audioâ†”text)                â”‚
        â”‚    + Residual Connections               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         Gated Fusion Module             â”‚
        â”‚    (Learnable combination weights)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         Regression Head (768-dim)       â”‚
        â”‚  768â†’512â†’384â†’256â†’128â†’64â†’1               â”‚
        â”‚  [Each: Linear+LN+GELU+Dropout]         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    OUTPUT: Grammar Score [1.0, 5.0]     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total Parameters: ~210M
Trainable Parameters: ~25M (12%)
```

---

## ğŸ“Š Results

### Cross-Validation Performance

| Fold | Pearson Correlation | RMSE |
|------|---------------------|------|
| 1    | 0.6484             | 0.6393 |
| 2    | 0.5674             | 0.6661 |
| 3    | 0.8306             | 0.5043 |
| 4    | 0.5603             | 0.6151 |
| 5    | 0.7714             | 0.4997 |
| **Mean** | **0.6756 Â± 0.1121** | **0.5849 Â± 0.0726** |

### Overall Metrics

- **Overall CV Pearson**: 0.6756
- **Overall CV RMSE**: 0.5849
- **Training Time**: ~2.5 hours on single GPU (T4)

### Score Distribution

```
Training Data Distribution:
  Mean: 3.42 | Std: 0.89 | Range: [1.0, 5.0]

Test Predictions Distribution:
  Mean: 3.38 | Std: 0.76 | Range: [1.2, 4.8]
```

---

## ğŸ”§ Installation

### Prerequisites

- Python 3.11+
- CUDA 11.8+ (for GPU support)
- 16GB+ RAM
- 8GB+ GPU memory (recommended)

### Setup

1. **Clone the repository**
```bash
git clone <repository-url>
cd grammar-score-prediction
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers datasets librosa soundfile scikit-learn pandas numpy matplotlib seaborn tqdm
```

4. **Download pre-trained models** (optional, will auto-download on first run)
```bash
# Models will be downloaded from HuggingFace Hub:
# - microsoft/wavlm-base
# - bert-base-uncased
```

---

## ğŸ’» Usage

### Training

1. **Update configuration in `train_grammar_model.py`**:

```python
CONFIG = {
    "train_audio_dir": "/path/to/train/audios",
    "test_audio_dir": "/path/to/test/audios",
    "train_csv": "/path/to/train.csv",
    "test_csv": "/path/to/test.csv",
    "train_transcripts_csv": "/path/to/train_transcripts.csv",
    "test_transcripts_csv": "/path/to/test_transcripts.csv",
    # ... other settings
}
```

2. **Run training**:

```bash
python train_grammar_model.py
```

**Output**:
- Trained model checkpoints: `model_fold_1.pt` through `model_fold_5.pt`
- Training plots: `output/multimodal_fold_*.png`
- CV summary: `output/cv_summary.png`

### Inference

Use the submission notebook `grammarscoreengine.ipynb` for generating predictions:

1. Open the notebook in Jupyter/Kaggle
2. Update model paths in the CONFIG section
3. Run all cells
4. Output: `submission.csv` with predictions

---

## ğŸ“ Project Structure

```
grammar-score-prediction/
â”‚
â”œâ”€â”€ train_grammar_model.py      # Training script (converted from notebook)
â”œâ”€â”€ grammarscoreengine.ipynb    # Submission notebook with inference
â”œâ”€â”€ README.md                    # This file
â”‚
â”œâ”€â”€ output/                      # Training outputs (auto-created)
â”‚   â”œâ”€â”€ model_fold_1.pt
â”‚   â”œâ”€â”€ model_fold_2.pt
â”‚   â”œâ”€â”€ model_fold_3.pt
â”‚   â”œâ”€â”€ model_fold_4.pt
â”‚   â”œâ”€â”€ model_fold_5.pt
â”‚   â”œâ”€â”€ cv_summary.png
â”‚   â”œâ”€â”€ multimodal_fold_1.png
â”‚   â”œâ”€â”€ multimodal_fold_2.png
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ data/                        # Dataset (not included in repo)
â”‚   â”œâ”€â”€ audios/
â”‚   â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â””â”€â”€ test/
â”‚   â””â”€â”€ csvs/
â”‚       â”œâ”€â”€ train.csv
â”‚       â”œâ”€â”€ test.csv
â”‚       â”œâ”€â”€ train_transcripts.csv
â”‚       â””â”€â”€ test_transcripts.csv
â”‚
â””â”€â”€ submission.csv               # Final predictions
```

---

## ğŸ§  Model Details

### Audio Processing

**Input**: WAV files (45-60 seconds, various sample rates)

**Preprocessing**:
1. Resample to 16kHz
2. Convert to mono
3. Peak normalization to [-1, 1]
4. Pad/crop to 10 seconds (160,000 samples)

**Feature Extraction**:
- WavLM-base encoder extracts contextualized audio representations
- Multi-head attention pooling aggregates temporal information
- Output: 768-dimensional audio embedding

### Text Processing

**Input**: Speech transcripts (from ASR system)

**Preprocessing**:
1. WordPiece tokenization (BERT tokenizer)
2. Truncate/pad to 512 tokens
3. Add special tokens: [CLS] ... [SEP]

**Feature Extraction**:
- BERT-base encoder produces contextual token embeddings
- Extract [CLS] token representation
- Output: 768-dimensional text embedding

### Fusion Mechanism

**Cross-Attention**:
```
Audio â†’ Query | Text â†’ Key, Value  â†’  Attended Audio
Text  â†’ Query | Audio â†’ Key, Value â†’  Attended Text
```

**Gated Fusion**:
```python
gate = Ïƒ(MLP([audio; text]))
fused = gate âŠ™ audio + (1-gate) âŠ™ text
```

This allows the model to learn which modality is more informative for each sample.

---

## ğŸ“ Training Details

### Hyperparameters

| Parameter         | Value                               |
|-------------------|-------------------------------------|
| Batch Size        | 6 (effective: 12 with accumulation) |
| Learning Rate     | 3e-5                                |
| Weight Decay      | 0.02                                |
| Optimizer         | AdamW (Î²1=0.9, Î²2=0.999)            |
| Scheduler         | OneCycleLR (warmup: 15%)            |
| Gradient Clipping | 1.0                                 |
| Epochs            | 25 (with early stopping)            |
| Mixed Precision   | FP16                                |
| SWA Start         | Epoch 12                            |

### Data Augmentation

Applied with 60% probability during training:

- **Time Stretching**: 0.9-1.1x (40% of augmented samples)
- **Pitch Shifting**: Â±5% (30%)
- **Gaussian Noise**: 0.001-0.008 level (50%)
- **Time Shifting**: Â±15% (40%)
- **Volume Perturbation**: 0.8-1.2x (50%)

### Computational Requirements

- **Training**: ~2.5 hours on NVIDIA T4 GPU
- **Memory**: 8GB GPU, 16GB RAM
- **Storage**: ~2GB for model checkpoints

---

## ğŸ”® Inference

### Ensemble Prediction

The final submission uses a **weighted ensemble** of all 5 fold models:

```python
# Weights based on fold Pearson correlations
weights = [0.6484, 0.5674, 0.8306, 0.5603, 0.7714]
weights = weights / sum(weights)  # Normalize

# Ensemble prediction
pred = sum(w * model_i(x) for w, model_i in zip(weights, models))
pred = clip(pred, 1.0, 5.0)
```

### Test-Time Augmentation (Optional)

For even more robust predictions:

```python
# Generate multiple augmented versions
preds = [model(augment(x)) for _ in range(3)]
final_pred = mean(preds)
```

---

## ğŸ“ˆ Evaluation

### Metrics

1. **Pearson Correlation** (Primary)
   - Measures linear relationship between predictions and ground truth
   - Range: [-1, 1], higher is better

2. **RMSE** (Root Mean Squared Error)
   - Measures prediction accuracy
   - Range: [0, âˆ), lower is better

### Validation Strategy

- **5-Fold Cross-Validation**: Ensures robust generalization
- **Stratified Splits**: Maintains label distribution across folds
- **Out-of-Fold Predictions**: Used for ensemble weight calibration

---

## ğŸš€ Future Improvements

### Architecture Enhancements

1. **Hierarchical Attention**: Better handling of long audio sequences
2. **Multi-Task Learning**: Joint prediction of fluency, pronunciation, vocabulary
3. **Uncertainty Quantification**: Bayesian approaches for confidence estimation
4. **Transformer XL**: Longer context modeling

### Training Improvements

1. **Curriculum Learning**: Train on easy samples first
2. **Focal Loss**: Focus on hard-to-classify samples
3. **Active Learning**: Select most informative samples for labeling
4. **Knowledge Distillation**: Compress ensemble into single model

### Data Enhancements

1. **Synthetic Data**: Generate augmented samples with TTS
2. **Back-Translation**: Text augmentation for better linguistic diversity
3. **External Data**: Leverage larger speech/grammar datasets
4. **Semi-Supervised Learning**: Use unlabeled audio samples

### Deployment

1. **Model Quantization**: INT8 for faster inference
2. **ONNX Export**: Framework-agnostic deployment
3. **REST API**: Web service for real-time scoring
4. **Mobile Optimization**: TensorFlow Lite for on-device inference

---

## ğŸ¯ Key Takeaways

### What Worked Well

âœ… **Multimodal Fusion**: Combining audio and text provides complementary information  
âœ… **Cross-Attention**: Allows modalities to interact and inform each other  
âœ… **SWA**: Improves generalization by averaging model weights  
âœ… **Mixed Precision**: Speeds up training without sacrificing quality  
âœ… **Ensemble**: 5-fold ensemble significantly reduces variance  

### Challenges Faced

âš ï¸ **Small Dataset**: 409 training samples limits model capacity  
âš ï¸ **Label Subjectivity**: Grammar scoring has inherent annotation variance  
âš ï¸ **Transcript Quality**: ASR errors can impact text branch performance  
âš ï¸ **Computational Cost**: Large models require significant GPU resources  

### Lessons Learned

ğŸ’¡ **Less is More**: Freezing early layers prevents overfitting on small data  
ğŸ’¡ **Augmentation Matters**: Audio augmentation crucial for generalization  
ğŸ’¡ **Loss Design**: Custom loss function tailored to RMSE metric improves results  
ğŸ’¡ **Careful Tuning**: Learning rate and scheduler critical for convergence  

---

## ğŸ“š References

### Models

- **WavLM**: [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900)
- **BERT**: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

### Techniques

- **SWA**: [Averaging Weights Leads to Wider Optima and Better Generalization](https://arxiv.org/abs/1803.05407)
- **Mixed Precision**: [Mixed Precision Training](https://arxiv.org/abs/1710.03740)
- **Cross-Attention**: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

---

## ğŸ‘¤ Author

**Aryan Verma**
- GitHub: [@Aryan-Verma-999](https://github.com/Aryan-Verma-999)
- Email: aryan-999@outlook.com